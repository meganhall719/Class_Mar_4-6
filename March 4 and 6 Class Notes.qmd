---
title: "March 4 Notes In Class"
format: html
editor: visual
---

## In Class March 4th

### In Class more on infer

offers a convenient set of functions and a standard worfklow for using permutation methods fro hypothesis testing, whether we are dealing with means, differences between means, proportions, or differences in proportions.

Specify something we are interested in in the data –\> hypothesize something on that data —\> generate data —\> calculate data —\> visualize

```{r}
library(dplyr)
library(tibble)
library(infer)
library(tidyverse)
f <-"https://raw.githubusercontent.com/difiore/ada-datasets/main/tbs-2006-2008-ranges.csv"
d <- read_csv(f, col_names = TRUE)
d 
head(d)
view(d)
```

1.  We first use the function *specify()* to indicate the variables we are interested in.

    ```{r}
    d <- d |>
    specify(formula = kernel95 ~ sex)
    ```

2.  The function hypothesize() is then used to declare the null hypothesis we wish to test.

    ```{r}
    d <- d |>
    hypothesize(null = "independence")
    ```

3.  We then use the *generate() function* to generate replicates of “permuted” data under the assumption that the null hypothesis is true.

    ```{r}
    perm <- d |>
      generate(reps = 1000, type = "permute")
    ```

4.  Next, we use calculate() to calculate summary statistics of interest for each replicate.

```{{r}}
perm <- perm |>
calculate(stat = "diff in means", order = c("M", "F"))

```

5.  We can then use the function visualize() to examine the null distribution

    ```{r}
    visualize(perm, bins = 20)

    ```

All together

```{r}
d <- d |>
specify(formula = kernel95 ~ sex)
d <- d |>
hypothesize(null = "independence")
perm <- d |>
  generate(reps = 10000, type = "permute")

perm <- perm |>
calculate(stat = "diff in means", order = c("M", "F"))
visualize(perm, bins = 20)
```

Visualize permutation distribution

-   observe

    ```{r}
    #observation
    obs <- d |>
    specify(kernel95 ~ sex) |>
    calculate(stat = "diff in means", order = c("M", "F"))
    #visualize 
    visualize(perm, bins = 20) +
    shade_p_value(obs_stat = obs, direction = "both")

    get_p_value(perm, obs, direction = "both")
    #pvalue very small we reject the null hypothesis that there is no association based on sex 

    ```

**permutation distribution should be 0 centered**

-   **take the mean of permutation**

```{r}
mean(perm$stat)
```

Another package called {modelr}

## Shifting gears : Regression

```{r}
library(tidyverse)
library(infer)
library(manipulate)
library(patchwork)
library(lmodel2)
library(sjPlot)
library(broom)
```

### [Regression: common form of data modeling]{.underline}

explore the relationship between an outcome variable (typically denotes as y)

[**Simple(general) Linear regression**]{.underline}

response variable is a typical numerical or categorical variable

[**Multiple (general) linear regression**]{.underline}

outcome is a continuous numerical variable, multiple predictors that are either numerical or categorical

[**ANOVA/ANCOVA**]{.underline}

focuses on categorical predictors

--\> Really focuses on categorical predictors

[**Generalized linear regression**]{.underline}

allows for binary categorical count variables as outcomes

--\> covid test postitive or negative

### Start with exploratory data analysis

Univariate summary statistics skim() {skimr}

### Bivariate summary statistics

-   **covaraince** how much two numerical variables "change together" and whether that change is positive or negative

-   **Correlation coefficient** is a standardized form of the vocation that summarizes, on a scale from -1 to +1 both the strength

-   **correlation coefficient** two is a standardize covariance (divide by the product of the standard deviations of the two variables

    ```{r}
    # loading dataset 
    f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
    d <- read_csv(f, col_names = TRUE)
    head(d)

    h <-d$height - mean(d$height)
    w<-d$weight - mean(d$weight)


    #plot the realationship between zombie apocalypse survivor weight and zombie apocalypse survivor height 
    cov <-sum(h*w)/(length(d$height)-1)


    # covaraince function 
    cov(d$height, d$weight)

    #correlation
    cor<-cov/(sd(d$height)*sd(d$weight))
    cor(d$height, d$weight)

    #plot
    plot(d$height,d$weight)
    ```

## Purpose for Regression

-   predict the value of an outcome variable y based on the information contained in a set of predictor variables x

-   describe and quantify the relationship between the outcome variable y and a set of explanatory variables z

-   develop and choose among different models

-   analyze co variation among sets of variables to identify/explore their relative explanatory power

-   determine causality

## Formula

model the outcome variable y "as a linear function" : of the explanatory/predictor variables

![](images/Screenshot%202025-03-04%20at%204.37.07%20PM.png){width="205" height="34"}

Beta values in this equation are referred to as "regression coefficients" and it is those coefficients that our analysis are trying to estimate. Usually trying to minimize error

Ordinary Least squares

```{r}
library(manipulate)
#fine the. line of best fit and minimizing the sum of squares in y varaible / response varaible

d <- mutate(d, centered_height = height - mean(height))
d <- mutate(d, centered_weight = weight - mean(weight))
mean(d$centered_height)

#slope.test 
slope.test <- function(beta1, data) {
    g <- ggplot(data = data, aes(x = centered_weight, y = centered_height))
    g <- g + geom_point()
    g <- g + geom_abline(intercept = 0, slope = beta1, size = 1, colour = "blue",
        alpha = 1/2)
    ols <- sum((data$centered_height - beta1 * data$centered_weight)^2)
    g <- g + ggtitle(paste("Slope = ", beta1, "\nSum of Squared Deviations = ", round(ols,
        3)))
    g
}

#manipulate statement
manipulate(slope.test(beta1, data = d), beta1 = slider(-1, 1, initial = 0, step = 0.005))
```

Analytically, for a uni variate regression (one predictor, one outcome variable) we can solve for beta coefficients

![](images/Screenshot%202025-03-04%20at%204.51.04%20PM.png)

```{r}
b1 <- (cor(d$centered_weight, d$centered_height) * sd(d$height))/((sd$centered_weight)))
b1
```

```{r}
b1 <- (cor(d$centered_weight, d$centered_height) * sd(d$height)) / (sd(d$centered_weight))
b1
```

```{r}
ssxy<-cov(d$centered_height, d$centered_weight)/(var(d$centered_height))
ssxy
```

```{r}
#Linear model 
m<-lm(height~wight, data = d)
```

## In Class March 6

Zombie apocalypse survivors dataset

```{r}
library(tidyverse)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
d <- read_csv(f, col_names = TRUE)
head(d)
plot(x= d$weight, y = d$height)

m<-lm(height~weight, data = d)
```

evaluate statistical evidence that there is a relationship between these variables. These next steps involve the process of statistical inference

```{r}
library(dplyr)
summary(m)
```

A lot of output in the summary:

first get ouput of the model we ran

get estimates of beta coefficients

also get adjusted r squared value ( taking amount of variance of the y variable that can be explained by the variance in the x variable

X value ; predicted y value through; if the actual y value is far enough tell you how good the fit.

R squared will be big if it data points fit the line of best fit well! the opposite is true

Tells you the number of degrees of freedom. ; n number of observations - 1 for each statistical analysis,

### other functions to run

run the command names() on the ooutput of lm

```{r}
names(m)
```

```{r}
m$coefficients

m$model

m$fitted.values #gives us predicted value of y "height"


m$residuals

head(m$residuals) #should be normally distributed tells us hoow good our fittted and real vaalues are wich was m$model

histogram(m$residuals) #does look normally distributed

qqplot(m) #plots standardized individuals lets you know if it follows a strraight line it is normally distributed. 
```

## use qq plot on exercise 8 !\*!

#### Broom package! I like it

```{r}
broom::tidy(m)
```

#### pulls out confidence intervals for you! How handy

```{r}
confint(m)

```

#### Calculating the SE in the regression coefficients

use standard error in B1 to get the standard error of B0

confirm you are getting what. r spits. out for u.

#### calculating T statistic and p value by hand

t = estimate/ standard error

p value = 2 \*\* pt (t, df = nrow-2)

```{r}

39.6/.596
confint(m)

2*pt(47.5, df = 998, lower.tail = FALSE)
```

pt gives you your t statistic

you can add them together ! Get the same answer I don't know why

```{r}
pt(47.5, df = 998, lower.tail = FALSE) + pt(-47.5, df= 998, lower.tail = TRUE)
```

```{r}
broom::glance(m)
```

### Key assumption of Linear Regression

-   The sample is represntative of the population and is unbiased

-   the predictor variables are measured with no error

-   residuals have an expected value(mean) of zero and are normally distributed ( use QQ plots, too describe the distribution

-   the relationship between the predictor variable and the response is not "linear"

    -   plot outcome versus predictor

-   plot rersidulas verrsus fitted values the variance of the rresidulas is constant across the. range of predictor variables (homoscedascity)

-   for multiple regression: predictors are not highly correlated height \~ weight and age

    -   if weight and age are highly correlated you cant use it

    -   examine correlation matrix

    -   compute variance inflation factors (VIFs), which measures how much multicollinearity increases when each predictor is added to a model

### Alternatives to Ordinary Leas Squares Regression

#### Model II regression approaches, a line of best fit is chosen that minimizes in some way

```{r}
broom::tidy(m)
plot(x=d$weight, y = d$height)
abline(coef(m), col = 'blue')

library(lmodel2)
m2 <-lmodel2(height ~weight, data = d, range.y = "relative", range.x = "relative", nperm = 1000)

summary(m2)

#putting a line of best fit through data 
betas <- broom::tidy(m2) |>
  filter(method == "OLS")|>
  pull(estimate)
abline(betas, col = "blue") #plotting on top of height and weight plot

betas <- broom::tidy(m2) |>
  filter(method == "RMA")|>
  pull(estimate)
abline(betas, col = "red")


```

### Maximum likelihood estimation

trying to figure out what parameter values are the most consistent given values we have

## Exercise 8

```{r}
f <-"https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/Street_et_al_2017.csv"
d <- read_csv(f, col_names = TRUE)
head(d)

#Plot brain size (ECV) as a function of social group size, longevity, juvenile period length, and reproductive lifespan (Repro_lifespan) (separate plots for each) 

library(ggplot2)
library(gridExtra)


par(mfrow = c(2, 2))
plot(d$Longevity, d$ECV)
plot(d$Group_size, d$ECV)
plot(d$Weaning, d$ECV)
plot(d$Repro_lifespan, d$ECV)
m1 <-lm(formula = ECV~Longevity, data = d)
m1


```

```{r}
par(mfrow = c(2, 2))
plot(d$Longevity, d$ECV)
plot(d$Group_size, d$ECV)
plot(d$Weaning, d$ECV)
plot(d$Repro_lifespan, d$ECV)
m1 <-lm(formula = ECV~Longevity, data = d)
m1

broom::tidy(m1)
confint(m1)

c <- d|>
  filter(Taxonomic_group == "Catarrhini")
c
m1 <-lm(formula = ECV~Longevity, data = c)
```
